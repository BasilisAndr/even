{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наброски кода для\n",
    "0. сведения алломорфов морфем в одно (хотя бы в таблице)\n",
    "1. подсчёта сочетаний морфем\n",
    "2. вытаскивания конвербов\n",
    "3. вытаскивания цепочек частей речи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все манипуляции можно тестировать на корпусах, которые в почте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0\n",
    "Действительно надо вытащить всё, положить в словарину и радоваться\n",
    "\n",
    "Формат:\n",
    "\n",
    "`корпус{\n",
    "    название:документ{    # done\n",
    "        мета:мета,    # done\n",
    "        текст:[\n",
    "            предложение{\n",
    "                слой:[\n",
    "                    морфемы\n",
    "                    ],\n",
    "                перевод:''\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora = ['Kamchatka', 'Sebjan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, re, pickle\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def morphs_2_words(line):\n",
    "    '''берёт на вход массив расчленённой строки, возвращает массив слов'''\n",
    "    words = []\n",
    "    word = []\n",
    "    for morph in line:\n",
    "        if morph.strip()[0] in '=-':\n",
    "            word.append(morph)\n",
    "        else:\n",
    "            if len(word) > 0:\n",
    "                words.append(word)\n",
    "            word = [morph]\n",
    "    words.append(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def lines_2_dict(part):\n",
    "    '''\n",
    "    i: кусок текста (предложение) в несколько строк, в каждой строке несколько слоёв, и с другими данными предложения\n",
    "    o: джейсонина вида {'слой': [сл, о, ва], 'слой': содержимое}\n",
    "    доп. ограничения: длина всех строк-массивов равна\n",
    "    '''\n",
    "    res = {}\n",
    "    # собирать по строке в словарь\n",
    "    # mad\n",
    "    lines = [line for line in part.split('\\n')[1:] if len(line) > 1] # leaving out the first line with index\n",
    "    parted_layers = ['tx', 'mb', 'ge', 'ps']\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "        layer = line[0].strip('\\\\')\n",
    "        if layer in parted_layers:\n",
    "            line_content = morphs_2_words(line[1:])\n",
    "        else:\n",
    "            line_content = ' '.join(line[1:])\n",
    "        if layer in res:\n",
    "            res[layer] += line_content\n",
    "        else:\n",
    "            res[layer] = line_content\n",
    "    return res\n",
    "\n",
    "\n",
    "def make_readable(corp):\n",
    "    '''\n",
    "    переводит текст корпусов в удобомашиночитаемую джейсонину.\n",
    "    '''\n",
    "    folder = 'Corpus_Text_{}_postagged'.format(corp)\n",
    "    corpus_dict = {}\n",
    "    for fil in os.listdir(folder):\n",
    "        if not fil.endswith('.txt'):\n",
    "            continue\n",
    "        with open(os.path.join(folder, fil), 'r') as f:\n",
    "            text = f.read()\n",
    "        file_content = {}\n",
    "        sents = text.split('\\id')\n",
    "        file_content['meta'] = sents[1] # metainfo at the beginning of the file; not parsed\n",
    "        text = []\n",
    "        for sent in sents[2:]:\n",
    "#             parts = re.findall('\\\\tx.*?\\n\\n', text, flags=re.DOTALL)\n",
    "            sent_content = lines_2_dict(sent)\n",
    "            text.append(sent_content)\n",
    "        file_content['text'] = text\n",
    "        corpus_dict[fil] = file_content\n",
    "    with open('{}.pickle'.format(corp), 'wb') as f:\n",
    "        pickle.dump(corpus_dict, f)\n",
    "    return corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect(string, layer, corpora=corpora, show=True):\n",
    "    for corpus in corpora:\n",
    "        hits = []\n",
    "        with open('{}.pickle'.format(corpus), 'rb') as f:\n",
    "            content = pickle.load(f)\n",
    "        for doc in content:\n",
    "            for i in range(len(content[doc]['text'])): # looping through sentences\n",
    "                if not layer in content[doc]['text'][i]:\n",
    "                    continue\n",
    "                layer = ' '.join([' '.join(x) for x in content[doc]['text'][i][layer]])\n",
    "                if string in layer:\n",
    "                    print('Found in {}, {} at {}'.format(corpus, doc, i))\n",
    "                    if show:\n",
    "                        pprint(content[doc]['text'][i])\n",
    "                    hits.append(doc)\n",
    "        if len(hits) == 0:\n",
    "            print('noth found. Cool!')\n",
    "        else:\n",
    "            inp = input('correct all these ({} hits) & press Enter and I\\'ll reload.\\n'.format(len(hits)))\n",
    "            if 'replace' in inp:\n",
    "                target = inp.split()[1]\n",
    "                for doc in list(set(hits)):\n",
    "                    with open(os.path.join('Corpus_Text_{}_postagged'.format(corpus), doc), 'r') as f:\n",
    "                        text = f.read()\n",
    "                    text = re.sub(string, target, text)\n",
    "                    with open(os.path.join('Corpus_Text_{}_postagged'.format(corpus), doc), 'w') as f:\n",
    "                        f.write(text)\n",
    "            make_readable(corpus)\n",
    "            print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for corp in corpora:\n",
    "    make_readable(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noth found. Cool!\n",
      "noth found. Cool!\n"
     ]
    }
   ],
   "source": [
    "detect('excl', 'ps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['v'], ['n'], ['onomat'], ['onomat'], ['n'], ['onomat'], ['onomat']]\n"
     ]
    }
   ],
   "source": [
    "with open('{}.pickle'.format('Kamchatka'), 'rb') as f:\n",
    "    content = pickle.load(f)\n",
    "pprint(content['Egorova_RM_Utakan.txt']['text'][22]['ps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "это уже начато там\n",
    "морфемы лежат в отдельном словаре, кажется; нужно приложить это знание на таблицу?\n",
    "или создать уже наконец промежуточное представление всего"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "сочетания морфем\n",
    "\n",
    "сначала надо вытащить представления (кажется это уже сделано)\n",
    "\n",
    "пройтись по всему, растащить каждое слово на пары морфем, первая (ROOT, morph), последняя (morph, END)\n",
    "\n",
    "`from collections import Counter\n",
    "# вытаскиваю все морфемы из первого слота, считаю, с чем они сочитаются, записываю в словарь\n",
    "morph_pairs_dict = {x: Counter([pair for pair in morph_pairs_list if pair[0]==x]) for x in set(map(lambda pair: pair[0], morph_pairs_list))}\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
