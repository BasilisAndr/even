{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наброски кода для\n",
    "0. сведения алломорфов морфем в одно (хотя бы в таблице)\n",
    "1. подсчёта сочетаний морфем\n",
    "2. вытаскивания конвербов\n",
    "3. вытаскивания цепочек частей речи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все манипуляции можно тестировать на корпусах, которые в почте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0\n",
    "Действительно надо вытащить всё, положить в словарину и радоваться\n",
    "\n",
    "Формат:\n",
    "\n",
    "`корпус{\n",
    "    название:документ{    # done\n",
    "        мета:мета,    # done\n",
    "        текст:[\n",
    "            предложение{\n",
    "                слой:[\n",
    "                    морфемы\n",
    "                    ],\n",
    "                перевод:''\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora = ['Kamchatka', 'Sebjan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, re, pickle\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def morphs_2_words(line):\n",
    "    '''берёт на вход массив расчленённой строки, возвращает массив слов'''\n",
    "    words = []\n",
    "    word = []\n",
    "    for morph in line:\n",
    "        if morph.strip()[0] in '=-':\n",
    "            word.append(morph)\n",
    "        else:\n",
    "            if len(word) > 0:\n",
    "                words.append(word)\n",
    "            word = [morph]\n",
    "    words.append(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def handle_startline(line, res, current_layer):\n",
    "    line = line.split()\n",
    "    layer = line[0].strip('\\\\')\n",
    "    parted_layers = ['tx', 'mb', 'ge', 'ps']\n",
    "    if not (len(line) == 1): # чтобы не считать пустые строки\n",
    "        if layer in parted_layers:\n",
    "            line_content = morphs_2_words(line[1:]) # делим на слова, состоящие из морфем\n",
    "        else:\n",
    "            line_content = [' '.join(line[1:])] # просто целые строки (комментарии и тп)\n",
    "            current_layer = layer\n",
    "        if layer in res and res[layer][0] != '':\n",
    "            res[layer] += line_content\n",
    "        else:\n",
    "            res[layer] = line_content\n",
    "    return res, current_layer\n",
    "\n",
    "\n",
    "def lines_2_dict(part):\n",
    "    '''\n",
    "    i: кусок текста (предложение) в несколько строк, в каждой строке несколько слоёв, и с другими данными предложения\n",
    "    o: джейсонина вида {'слой': [сл, о, ва], 'слой': содержимое}\n",
    "    доп. ограничения: длина всех строк-массивов равна\n",
    "    '''\n",
    "    res = {}\n",
    "    lines = [line for line in part.split('\\n') if len(line) > 1]\n",
    "    res['index'] = [lines[0].split('_')[-1]]\n",
    "    parted_layers = ['tx', 'mb', 'ge', 'ps']\n",
    "    current_layer = '' # для переносов\n",
    "    for line in lines[1:]:\n",
    "        if line.startswith('\\\\'):\n",
    "            res, current_layer = handle_startline(line, res, current_layer)\n",
    "        else:\n",
    "            if current_layer:\n",
    "                res[current_layer][0] += ' ' + line\n",
    "    return res\n",
    "\n",
    "\n",
    "def check_len(p_sent, fil):\n",
    "    parted_layers = ['mb', 'ge', 'ps']\n",
    "    selected_layers = [key for key in p_sent if key in parted_layers and len(p_sent[key])>1]\n",
    "    # 1. check that the number of words is the same\n",
    "    lengths = set([len(p_sent[key]) for key in selected_layers])\n",
    "    if len(lengths) > 1:\n",
    "        print('Error in {}, here:'.format(fil))\n",
    "        print(lengths)\n",
    "        for l in selected_layers:\n",
    "            print(len(p_sent[l]))\n",
    "            pprint(p_sent[l])\n",
    "        return lengths\n",
    "    # 2. check that morphemes are aligned\n",
    "    if 'mb' in selected_layers and 'ge' in selected_layers:\n",
    "        for i in range(len(p_sent['ge'])):\n",
    "            if len(p_sent['ge'][i]) != len(p_sent['mb'][i]):\n",
    "                print('што-то слиплось в {}'.format(fil))\n",
    "                pprint(p_sent)\n",
    "    return lengths\n",
    "\n",
    "\n",
    "def make_readable(corp):\n",
    "    \"переводит текст корпусов в удобомашиночитаемую джейсонину\"\n",
    "    folder = 'Corpus_Text_{}_postagged'.format(corp)\n",
    "    corpus_dict = {}\n",
    "    for fil in os.listdir(folder):\n",
    "        if 'pyzhik' in fil:\n",
    "            continue\n",
    "        if not fil.endswith('.txt'):\n",
    "            continue\n",
    "        with open(os.path.join(folder, fil), 'r') as f:\n",
    "            text = f.read()\n",
    "        file_content, text_content = {}, []\n",
    "        sents = text.split('\\id')\n",
    "        file_content['meta'] = sents[1] # metainfo at the beginning of the file; not parsed\n",
    "        for sent in sents[2:]:\n",
    "            sent_content = lines_2_dict(sent)\n",
    "            check_len(sent_content, fil)\n",
    "            text_content.append(sent_content)\n",
    "        file_content['text'] = text_content\n",
    "        corpus_dict[fil] = file_content\n",
    "    with open('{}.pickle'.format(corp), 'wb') as f:\n",
    "        pickle.dump(corpus_dict, f)\n",
    "    return corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for corp in corpora:\n",
    "    make_readable(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect(what, where, replace=False, corpora=corpora, show=True):\n",
    "    for corpus in corpora:\n",
    "        hits = []\n",
    "        with open('{}.pickle'.format(corpus), 'rb') as f:\n",
    "            content = pickle.load(f)\n",
    "        for doc in content:\n",
    "            for i in range(len(content[doc]['text'])): # looping through sentences\n",
    "                if not where in content[doc]['text'][i]:\n",
    "                    continue\n",
    "                for word in content[doc]['text'][i][where]:\n",
    "                    if what in word:\n",
    "                        print('Found in {}, {} at {}'.format(corpus, doc, i))\n",
    "                        if show:\n",
    "                            pprint(content[doc]['text'][i])\n",
    "                        hits.append(doc)\n",
    "        if len(hits) == 0:\n",
    "            print('noth found. cool! or not?')\n",
    "        else:\n",
    "            inp = input('correct all these ({} hits) & press Enter and I\\'ll reload.\\n'.format(len(hits)))\n",
    "            if replace:\n",
    "                target = replace\n",
    "                for doc in list(set(hits)):\n",
    "                    with open(os.path.join('Corpus_Text_{}_postagged'.format(corpus), doc), 'r') as f:\n",
    "                        text = f.read()\n",
    "                    a_hits = re.findall('{}\\\\b'.format(what), text)\n",
    "                    while len(a_hits) > len(hits):\n",
    "                        print('what is not accurate. see what\\'d be affected')\n",
    "                        pprint(a_hits[:15])\n",
    "                        what = input('change the what: ')\n",
    "                        a_hits = re.findall('{} '.format(what.replace('.', '\\.')), text)\n",
    "                    text = re.sub('{}(\\\\b)'.format(what.replace('.', '\\.')), '{}\\\\1'.format(target), text)\n",
    "                    with open(os.path.join('Corpus_Text_{}_postagged'.format(corpus), doc), 'w') as f:\n",
    "                        f.write(text)\n",
    "            make_readable(corpus)\n",
    "            print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "\n",
      "\n",
      "regalar nfs done\n",
      "\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "Found in Sebjan, SlepcovaNA_her_class.txt at 27\n",
      "Found in Sebjan, Zaxarova_JP_pear_story.txt at 21\n",
      "correct all these (2 hits) & press Enter and I'll reload.\n",
      "\n",
      "done!\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n",
      "Found in Sebjan, Krivoshapkina_AE_childhood.txt at 114\n",
      "Found in Sebjan, Krivoshapkin_DM_Segen.txt at 40\n",
      "Found in Sebjan, Krivoshapkin_DM_Segen.txt at 40\n",
      "correct all these (3 hits) & press Enter and I'll reload.\n",
      "\n",
      "done!\n",
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n"
     ]
    }
   ],
   "source": [
    "nfs = ['-nonfut3pl', '-nonfut3sg', '-nonfut.3pl', '-nonfut.3sg']\n",
    "for nf in nfs:\n",
    "    detect(nf, 'ge', replace='-nonfut', show=False)\n",
    "print('\\n\\nregalar nfs done\\n')\n",
    "other_nfs = ['-hab.nonfut3pl', '-hab.nonfut3sg', '-hab.nonfut.3pl', '-hab.nonfut.3sg']\n",
    "for nf in other_nfs:\n",
    "    detect(nf, 'ge', replace='-hab.nonfut', show=False)\n",
    "other_nfs = ['-caus.nonfut3pl', '-caus.nonfut3sg', '-caus.nonfut.3pl', '-caus.nonfut.3sg']\n",
    "for nf in other_nfs:\n",
    "    detect(nf, 'ge', replace='-caus.nonfut', show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь повторяю таблицу которая была"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_morphs(corpus):\n",
    "    with open('{}.pickle'.format(corpus), 'rb') as f:\n",
    "        content = pickle.load(f)\n",
    "    morphs = []\n",
    "    for doc in content:\n",
    "        for sent in doc['text']:\n",
    "            if 'mb' in sent and 'ge' in sent and 'ps' in sent:\n",
    "                morphs.append(('ROOT'))\n",
    "                for i in range(len(sent['mb'])):\n",
    "                    ps = sent['ps'][i]\n",
    "                    for l in range(len(sent['mb'][i])):\n",
    "                        morphs.append((sent['mb'][i][l], sent['ge'][i][l], ps))\n",
    "                morphs.append(('END'))\n",
    "    with open('{}_morphemes.pickle'.format(corpus), 'wb') as f:\n",
    "        pickle.dump(morphs, f)\n",
    "    print(len(morphs))\n",
    "    pprint(morphs[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-73dd9e167755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcorp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mextract_morphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-7e64f8b124c8>\u001b[0m in \u001b[0;36mextract_morphs\u001b[0;34m(corpus)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmorphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'mb'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'ge'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'ps'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mmorphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ROOT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "for corp in corpora:\n",
    "    extract_morphs(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "это уже начато там\n",
    "морфемы лежат в отдельном словаре, кажется; нужно приложить это знание на таблицу?\n",
    "или создать уже наконец промежуточное представление всего"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "сочетания морфем\n",
    "\n",
    "сначала надо вытащить представления (кажется это уже сделано)\n",
    "\n",
    "пройтись по всему, растащить каждое слово на пары морфем, первая (ROOT, morph), последняя (morph, END)\n",
    "\n",
    "`from collections import Counter\n",
    "# вытаскиваю все морфемы из первого слота, считаю, с чем они сочитаются, записываю в словарь\n",
    "morph_pairs_dict = {x: Counter([pair for pair in morph_pairs_list if pair[0]==x]) for x in set(map(lambda pair: pair[0], morph_pairs_list))}\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
