{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наброски кода для\n",
    "0. сведения алломорфов морфем в одно (хотя бы в таблице)\n",
    "1. подсчёта сочетаний морфем\n",
    "2. вытаскивания конвербов\n",
    "3. вытаскивания цепочек частей речи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все манипуляции можно тестировать на корпусах, которые в почте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0\n",
    "Действительно надо вытащить всё, положить в словарину и радоваться\n",
    "\n",
    "Формат:\n",
    "\n",
    "`корпус{\n",
    "    название:документ{    # done\n",
    "        мета:мета,    # done\n",
    "        текст:[\n",
    "            предложение{\n",
    "                слой:[\n",
    "                    морфемы\n",
    "                    ],\n",
    "                перевод:''\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora = ['Kamchatka', 'Sebjan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, re, pickle\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def morphs_2_words(line):\n",
    "    '''берёт на вход массив расчленённой строки, возвращает массив слов'''\n",
    "    words = []\n",
    "    word = []\n",
    "    for morph in line:\n",
    "        if morph.strip()[0] in '=-':\n",
    "            word.append(morph)\n",
    "        else:\n",
    "            if len(word) > 0:\n",
    "                words.append(word)\n",
    "            word = [morph]\n",
    "    words.append(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def handle_startline(line, res, current_layer):\n",
    "    line = line.split()\n",
    "    layer = line[0].strip('\\\\')\n",
    "    if not (len(line) == 1): # чтобы не считать пустые строки\n",
    "        if layer in parted_layers:\n",
    "            line_content = morphs_2_words(line[1:]) # делим на слова, состоящие из морфем\n",
    "        else:\n",
    "            line_content = [' '.join(line[1:])] # просто целые строки (комментарии и тп)\n",
    "            current_layer = layer\n",
    "        if layer in res and res[layer][0] != '':\n",
    "            res[layer] += line_content\n",
    "        else:\n",
    "            res[layer] = line_content\n",
    "    return res, current_layer\n",
    "\n",
    "\n",
    "def lines_2_dict(part): # FIX EMPTY LINES\n",
    "    '''\n",
    "    i: кусок текста (предложение) в несколько строк, в каждой строке несколько слоёв, и с другими данными предложения\n",
    "    o: джейсонина вида {'слой': [сл, о, ва], 'слой': содержимое}\n",
    "    доп. ограничения: длина всех строк-массивов равна\n",
    "    '''\n",
    "    res = {}\n",
    "    lines = [line for line in part.split('\\n') if len(line) > 1]\n",
    "    res['index'] = [lines[0].split('_')[-1]]\n",
    "    parted_layers = ['tx', 'mb', 'ge', 'ps']\n",
    "    current_layer = '' # для переносов\n",
    "    for line in lines[1:]:\n",
    "        if line.startswith('\\\\'):\n",
    "            res, current_layer = handle_startline(line, res, current_layer)\n",
    "        else:\n",
    "            if current_layer:\n",
    "                res[current_layer][0] += ' ' + line\n",
    "    return res\n",
    "\n",
    "\n",
    "def check_len(p_sent, fil):\n",
    "    parted_layers = ['mb', 'ge', 'ps']\n",
    "    selected_layers = [key for key in p_sent if key in parted_layers and len(p_sent[key])>1]\n",
    "    # 1. check that the number of words is the same\n",
    "    lengths = set([len(p_sent[key]) for key in selected_layers])\n",
    "    if len(lengths) > 1:\n",
    "        print('Error in {}, here:'.format(fil))\n",
    "        print(lengths)\n",
    "        for l in selected_layers:\n",
    "            print(len(p_sent[l]))\n",
    "            pprint(p_sent[l])\n",
    "        return lengths\n",
    "    # 2. check that morphemes are aligned\n",
    "    if 'mb' in selected_layers and 'ge' in selected_layers:\n",
    "        for i in range(len(p_sent['ge'])):\n",
    "            if len(p_sent['ge'][i]) != len(p_sent['mb'][i]):\n",
    "                print('што-то слиплось в {}'.format(fil))\n",
    "                pprint(p_sent)\n",
    "    return lengths\n",
    "\n",
    "\n",
    "def make_readable(corp):\n",
    "    \"переводит текст корпусов в удобомашиночитаемую джейсонину\"\n",
    "    folder = 'Corpus_Text_{}_postagged'.format(corp)\n",
    "    corpus_dict = {}\n",
    "    for fil in os.listdir(folder):\n",
    "        if not fil.endswith('.txt'):\n",
    "            continue\n",
    "        with open(os.path.join(folder, fil), 'r') as f:\n",
    "            text = f.read()\n",
    "        file_content, text_content = {}, []\n",
    "        sents = text.split('\\id')\n",
    "        file_content['meta'] = sents[1] # metainfo at the beginning of the file; not parsed\n",
    "        for sent in sents[2:]:\n",
    "            sent_content = lines_2_dict(sent)\n",
    "            check_len(sent_content, fil)\n",
    "            text_content.append(sent_content)\n",
    "        file_content['text'] = text_content\n",
    "        corpus_dict[fil] = file_content\n",
    "    with open('{}.pickle'.format(corp), 'wb') as f:\n",
    "        pickle.dump(corpus_dict, f)\n",
    "    return corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for corp in corpora:\n",
    "#     make_readable(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect(what, where, corpora=corpora, show=True):\n",
    "    for corpus in corpora:\n",
    "        hits = []\n",
    "        with open('{}.pickle'.format(corpus), 'rb') as f:\n",
    "            content = pickle.load(f)\n",
    "        for doc in content:\n",
    "            for i in range(len(content[doc]['text'])): # looping through sentences\n",
    "                if not where in content[doc]['text'][i]:\n",
    "                    continue\n",
    "                for word in content[doc]['text'][i][where]:\n",
    "                    if what in word:\n",
    "                        print('Found in {}, {} at {}'.format(corpus, doc, i))\n",
    "                        if show:\n",
    "                            pprint(content[doc]['text'][i])\n",
    "                        hits.append(doc)\n",
    "        if len(hits) == 0:\n",
    "            print('noth found. cool! or not?')\n",
    "        else:\n",
    "            inp = input('correct all these ({} hits) & press Enter and I\\'ll reload.\\n'.format(len(hits)))\n",
    "            if 'replace' in inp:\n",
    "                target = inp.split()[1]\n",
    "                for doc in list(set(hits)):\n",
    "                    with open(os.path.join('Corpus_Text_{}_postagged'.format(corpus), doc), 'r') as f:\n",
    "                        text = f.read()\n",
    "                    text = re.sub('{} '.format(what), '{} '.format(target), text)\n",
    "                    with open(os.path.join('Corpus_Text_{}_postagged'.format(corpus), doc), 'w') as f:\n",
    "                        f.write(text)\n",
    "            make_readable(corpus)\n",
    "            print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noth found. cool! or not?\n",
      "noth found. cool! or not?\n"
     ]
    }
   ],
   "source": [
    "detect('=2sg\\ft', 'ge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('{}.pickle'.format('Sebjan'), 'rb') as f:\n",
    "#     content = pickle.load(f)\n",
    "# pprint(content['Alekseeva_RD_lost_tapes_znatoki_NA.txt']['text'][55]['ge'])\n",
    "fil = 'Kejmetinova_AA_headmistress_Yakutsk_310310_LZ.txt'\n",
    "with open(os.path.join('Corpus_Text_Sebjan_postagged', fil), 'r') as f:\n",
    "    text = f.read()\n",
    "sents = text.split('\\id')\n",
    "for i in range(len(sents)):\n",
    "    sent_content = lines_2_dict(sents[i])\n",
    "    check_len(sent_content, fil)\n",
    "# part = sents[27]\n",
    "# res = {}\n",
    "# lines = [line for line in part.split('\\n') if len(line) > 1]\n",
    "# res['index'] = [lines[0].split('_')[-1]]\n",
    "# parted_layers = ['tx', 'mb', 'ge', 'ps']\n",
    "# current_layer = '' # для переносов\n",
    "# for line in lines[1:]:\n",
    "#     if line.startswith('\\\\'):\n",
    "#         line = line.split()\n",
    "#         layer = line[0].strip('\\\\')\n",
    "#         if not (len(line) == 1): # чтобы не считать пустые строки\n",
    "#             print(line)\n",
    "#             if layer in parted_layers:\n",
    "#                 line_content = morphs_2_words(line[1:]) # делим на слова, состоящие из морфем\n",
    "#             else:\n",
    "#                 line_content = [' '.join(line[1:])] # просто целые строки (комментарии и тп)\n",
    "#                 current_layer = layer\n",
    "#             if layer in res and res[layer][0] != '':\n",
    "#                 res[layer] += line_content\n",
    "#             else:\n",
    "#                 res[layer] = line_content\n",
    "#     else:\n",
    "#         if current_layer:\n",
    "#             res[current_layer][0] += ' ' + line\n",
    "# pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "это уже начато там\n",
    "морфемы лежат в отдельном словаре, кажется; нужно приложить это знание на таблицу?\n",
    "или создать уже наконец промежуточное представление всего"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "сочетания морфем\n",
    "\n",
    "сначала надо вытащить представления (кажется это уже сделано)\n",
    "\n",
    "пройтись по всему, растащить каждое слово на пары морфем, первая (ROOT, morph), последняя (morph, END)\n",
    "\n",
    "`from collections import Counter\n",
    "# вытаскиваю все морфемы из первого слота, считаю, с чем они сочитаются, записываю в словарь\n",
    "morph_pairs_dict = {x: Counter([pair for pair in morph_pairs_list if pair[0]==x]) for x in set(map(lambda pair: pair[0], morph_pairs_list))}\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
