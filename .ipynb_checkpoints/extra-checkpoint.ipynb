{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наброски кода для\n",
    "0. сведения алломорфов морфем в одно (хотя бы в таблице)\n",
    "1. подсчёта сочетаний морфем\n",
    "2. вытаскивания конвербов\n",
    "3. вытаскивания цепочек частей речи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все манипуляции можно тестировать на корпусах, которые в почте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0\n",
    "Действительно надо вытащить всё, положить в словарину и радоваться\n",
    "\n",
    "Формат:\n",
    "\n",
    "`корпус{\n",
    "    название:документ{    # done\n",
    "        мета:мета,    # done\n",
    "        текст:[\n",
    "            предложение{\n",
    "                слой:[\n",
    "                    морфемы\n",
    "                    ],\n",
    "                перевод:''\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora = ['Kamchatka', 'Sebjan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, re, pickle\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def morphs_2_words(line):\n",
    "    '''берёт на вход массив расчленённой строки, возвращает массив слов'''\n",
    "    words = []\n",
    "    word = []\n",
    "    for morph in line:\n",
    "        if morph.strip()[0] in '=-':\n",
    "            word.append(morph)\n",
    "        else:\n",
    "            if len(word) > 0:\n",
    "                words.append(word)\n",
    "            word = [morph]\n",
    "    words.append(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def check_len(p_sent):\n",
    "    lengths = [len(p_sent[key]) for key in p_sent if len(p_sent[key])>1]\n",
    "    first = lengths[0]\n",
    "    for i in lengths:\n",
    "        if i != first:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def lines_2_dict(part):\n",
    "    '''\n",
    "    i: кусок текста (предложение) в несколько строк, в каждой строке несколько слоёв, и с другими данными предложения\n",
    "    o: джейсонина вида {'слой': [сл, о, ва], 'слой': содержимое}\n",
    "    доп. ограничения: длина всех строк-массивов равна\n",
    "    '''\n",
    "    res = {}\n",
    "    lines = [line for line in part.split('\\n') if len(line) > 1]\n",
    "    res['index'] = lines[0].split('_')[-1]\n",
    "    parted_layers = ['tx', 'mb', 'ge', 'ps']\n",
    "    for line in lines[1:]:\n",
    "        line = line.split()\n",
    "        layer = line[0].strip('\\\\')\n",
    "        if layer in parted_layers:\n",
    "            line_content = morphs_2_words(line[1:])\n",
    "        else:\n",
    "            line_content = ' '.join(line[1:])\n",
    "        if layer in res:\n",
    "            res[layer] += line_content\n",
    "        else:\n",
    "            res[layer] = line_content\n",
    "    if not check_len(res):\n",
    "        res = False\n",
    "    return res\n",
    "\n",
    "\n",
    "def make_readable(corp):\n",
    "    \"переводит текст корпусов в удобомашиночитаемую джейсонину\"\n",
    "    folder = 'Corpus_Text_{}_postagged'.format(corp)\n",
    "    corpus_dict = {}\n",
    "    for fil in os.listdir(folder):\n",
    "        if not fil.endswith('.txt'):\n",
    "            continue\n",
    "        with open(os.path.join(folder, fil), 'r') as f:\n",
    "            text = f.read()\n",
    "        file_content = {}\n",
    "        sents = text.split('\\id')\n",
    "        file_content['meta'] = sents[1] # metainfo at the beginning of the file; not parsed\n",
    "        text = []\n",
    "        for sent in sents[2:]:\n",
    "            sent_content = lines_2_dict(sent)\n",
    "            if not sent_content:\n",
    "                print('Error in {}, here:'.format(fil))\n",
    "                pprint(sent)\n",
    "                break\n",
    "            text.append(sent_content)\n",
    "        file_content['text'] = text\n",
    "        corpus_dict[fil] = file_content\n",
    "    with open('{}.pickle'.format(corp), 'wb') as f:\n",
    "        pickle.dump(corpus_dict, f)\n",
    "    return corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-6661510aac52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcorp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmake_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-115-3e7bc37e2a0f>\u001b[0m in \u001b[0;36mmake_readable\u001b[0;34m(corp)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0msent_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines_2_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msent_content\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error in {}, here:'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-3e7bc37e2a0f>\u001b[0m in \u001b[0;36mlines_2_dict\u001b[0;34m(part)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-3e7bc37e2a0f>\u001b[0m in \u001b[0;36mcheck_len\u001b[0;34m(p_sent)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "for corp in corpora:\n",
    "    make_readable(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect(what, where, corpora=corpora, show=True):\n",
    "    for corpus in corpora:\n",
    "        hits = []\n",
    "        with open('{}.pickle'.format(corpus), 'rb') as f:\n",
    "            content = pickle.load(f)\n",
    "        for doc in content:\n",
    "            for i in range(len(content[doc]['text'])): # looping through sentences\n",
    "                if not where in content[doc]['text'][i]:\n",
    "                    continue\n",
    "                layer = ' '.join([' '.join(x) for x in content[doc]['text'][i][where]])\n",
    "                if what in layer:\n",
    "                    print('Found in {}, {} at {}'.format(corpus, doc, i))\n",
    "                    if show:\n",
    "                        print(content[doc]['text'][i])\n",
    "                    hits.append(doc)\n",
    "        if len(hits) == 0:\n",
    "            print('noth found. cool! or not?')\n",
    "        else:\n",
    "            inp = input('correct all these ({} hits) & press Enter and I\\'ll reload.\\n'.format(len(hits)))\n",
    "            if 'replace' in inp:\n",
    "                target = inp.split()[1]\n",
    "                for doc in list(set(hits)):\n",
    "                    with open(os.path.join('Corpus_Text_{}_postagged'.format(corpus), doc), 'r') as f:\n",
    "                        text = f.read()\n",
    "                    text = re.sub(what, target, text)\n",
    "                    with open(os.path.join('Corpus_Text_{}_postagged'.format(corpus), doc), 'w') as f:\n",
    "                        f.write(text)\n",
    "            make_readable(corpus)\n",
    "            print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in Sebjan, Alekseeva_RD_lost_tapes_znatoki_NA.txt at 55\n",
      "{'ELANBegin': '00:04:34.530',\n",
      " 'ELANEnd': '00:04:44.570',\n",
      " 'ELANParticipant': '',\n",
      " 'ft': 'Once also, sometimes we also sit , talking, remembering old times.',\n",
      " 'ge': [['dist'],\n",
      "        ['also.Y'],\n",
      "        ['one', '-iter.adv'],\n",
      "        ['also.Y'],\n",
      "        ['sit.down', '-ep', '-res', '-sim.cvb'],\n",
      "        ['front', '-adjr'],\n",
      "        ['what', '-acc'],\n",
      "        ['remember', '-sim.cvask'],\n",
      "        ['?', '-ipf???'],\n",
      "        ['prox.qual'],\n",
      "        ['tell', '-rec', '-sim.cvb'],\n",
      "        ['sometimes'],\n",
      "        ['sit.down', '-ep', '-res', '-hab', '-nonfut', '-1pl.in']],\n",
      " 'index': '056',\n",
      " 'mb': [['tarak'],\n",
      "        ['emie'],\n",
      "        ['omen', '-REkEn'],\n",
      "        ['emie'],\n",
      "        ['teg', '-E', '-Č', '-nIkEn'],\n",
      "        ['dʒul', '-(E)rEp'],\n",
      "        ['ịak', '-W'],\n",
      "        ['dʒọn', '-nIkEn'],\n",
      "        ['ulgim', '-i'],\n",
      "        ['erroːčin'],\n",
      "        ['ukčen', '-mEČ', '-nIkEn'],\n",
      "        ['haːdụn'],\n",
      "        ['teg', '-E', '-Č', '-Gr(E)', '-R(E)', '-p']],\n",
      " 'ph': 'emneken = omneken',\n",
      " 'ps': [['pron'],\n",
      "        ['adv'],\n",
      "        ['num'],\n",
      "        ['adv'],\n",
      "        ['v'],\n",
      "        ['rel.n'],\n",
      "        ['pron'],\n",
      "        ['v'],\n",
      "        ['?'],\n",
      "        ['pron'],\n",
      "        ['v'],\n",
      "        ['adv'],\n",
      "        ['v']],\n",
      " 'ru': 'Однажды тоже, сидя, прежнее вспоминая, так разговаривая, тоже',\n",
      " 'tx': [['Tara'],\n",
      "        ['emie'],\n",
      "        ['omneken'],\n",
      "        ['emie'],\n",
      "        ['tegetniken'],\n",
      "        ['dʒulep'],\n",
      "        ['ịaw'],\n",
      "        ['dʒọnịkan,'],\n",
      "        ['ulgimin...'],\n",
      "        ['ečimur'],\n",
      "        ['ukčemmetniken'],\n",
      "        ['haːdụn'],\n",
      "        ['tegekkererep.']],\n",
      " 'иногда': 'сидим.'}\n",
      "correct all these (1 hits) & press Enter and I'll reload.\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "detect('sim.cvask', 'ge', ['Sebjan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dist'],\n",
      " ['also.Y'],\n",
      " ['one', '-iter.adv'],\n",
      " ['also.Y'],\n",
      " ['sit.down', '-ep', '-res', '-sim.cvb'],\n",
      " ['front', '-adjr'],\n",
      " ['what', '-acc'],\n",
      " ['remember', '-sim.cvask'],\n",
      " ['?', '-ipf???'],\n",
      " ['prox.qual'],\n",
      " ['tell', '-rec', '-sim.cvb'],\n",
      " ['sometimes'],\n",
      " ['sit.down', '-ep', '-res', '-hab', '-nonfut', '-1pl.in']]\n",
      "('dist also.Y one -iter.adv also.Y sit.down -ep -res -sim.cvb front -adjr what '\n",
      " '-acc remember -sim.cvask ? -ipf??? prox.qual tell -rec -sim.cvb sometimes '\n",
      " 'sit.down -ep -res -hab -nonfut -1pl.in')\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with open('{}.pickle'.format('Sebjan'), 'rb') as f:\n",
    "    content = pickle.load(f)\n",
    "pprint(content['Alekseeva_RD_lost_tapes_znatoki_NA.txt']['text'][55]['ge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "это уже начато там\n",
    "морфемы лежат в отдельном словаре, кажется; нужно приложить это знание на таблицу?\n",
    "или создать уже наконец промежуточное представление всего"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "сочетания морфем\n",
    "\n",
    "сначала надо вытащить представления (кажется это уже сделано)\n",
    "\n",
    "пройтись по всему, растащить каждое слово на пары морфем, первая (ROOT, morph), последняя (morph, END)\n",
    "\n",
    "`from collections import Counter\n",
    "# вытаскиваю все морфемы из первого слота, считаю, с чем они сочитаются, записываю в словарь\n",
    "morph_pairs_dict = {x: Counter([pair for pair in morph_pairs_list if pair[0]==x]) for x in set(map(lambda pair: pair[0], morph_pairs_list))}\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
